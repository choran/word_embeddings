{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Word2Vec model\n",
    "This code can be found in the TensorFlow tutorial code [here](https://www.tensorflow.org/tutorials/text/word2vec) <br>\n",
    "There is much more informational and educational detail in the TF tutorial so make sure take a look at that. <br>\n",
    "We will just use the key parts of the code here so that we can focus on what we need too product static word embeddings <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import psutil\n",
    "\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dot, Embedding, Flatten, GlobalAveragePooling1D, Reshape\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you are running TF version 2 or later\n",
    "# You can find installation steps here: https://www.tensorflow.org/install\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42 \n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "# Set the number of negative samples per positive context. \n",
    "num_ns = 4\n",
    "# Set you API token as an env variable or else (not recommended) just add it directly to your notebook\n",
    "NEPTUNE_API_TOKEN = (os.getenv('NEPTUNE_API'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your neptune credentials \n",
    "This will enable you to track your experiment"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# You may need to install these modules if you have not already done so\n",
    "!pip install neptune-client\n",
    "!pip install neptune-contrib\n",
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "from neptunecontrib.api import log_table\n",
    "from neptunecontrib.api import log_chart\n",
    "\n",
    "neptune.init(project_qualified_name='<username>/sandbox',\n",
    "             api_token=NEPTUNE_API_TOKEN, \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune.create_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the home dir\n",
    "This is where you are running your notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aj--8RFK6fgW"
   },
   "source": [
    "### Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T17:07:18.248391Z",
     "iopub.status.busy": "2020-10-30T17:07:18.247479Z",
     "iopub.status.idle": "2020-10-30T17:07:18.249239Z",
     "shell.execute_reply": "2020-10-30T17:07:18.249634Z"
    },
    "id": "63INISDEX1Hu"
   },
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for vocab_size tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence, \n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "    \n",
    "    # Iterate over each positive skip-gram pair to produce training examples \n",
    "    # with positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1, \n",
    "          num_sampled=num_ns, \n",
    "          unique=True, \n",
    "          range_max=vocab_size, \n",
    "          seed=SEED, \n",
    "          name=\"negative_sampling\")\n",
    "      \n",
    "      # Build context and label vectors (for one target word)\n",
    "      negative_sampling_candidates = tf.expand_dims(\n",
    "          negative_sampling_candidates, 1)\n",
    "\n",
    "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shvPC8Ji2cMK"
   },
   "source": [
    "## Prepare training data for Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFlikI6L26nh"
   },
   "source": [
    "### Get the data we generated for out test lanugage dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = f'{HOME_DIR}/proximity_vocab.txt'\n",
    "# path_to_file = f'{HOME_DIR}/random_vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T17:07:18.286372Z",
     "iopub.status.busy": "2020-10-30T17:07:18.285742Z",
     "iopub.status.idle": "2020-10-30T17:07:18.295163Z",
     "shell.execute_reply": "2020-10-30T17:07:18.294695Z"
    },
    "id": "lfgnsUw3ofMD",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(path_to_file) as f: \n",
    "  lines = f.read().splitlines()\n",
    "for line in lines[:5]:\n",
    "  print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTNZYqUs5C2V"
   },
   "source": [
    "Use the non empty lines to construct a `tf.data.TextLineDataset` object for next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T17:07:18.303003Z",
     "iopub.status.busy": "2020-10-30T17:07:18.299576Z",
     "iopub.status.idle": "2020-10-30T17:07:18.359672Z",
     "shell.execute_reply": "2020-10-30T17:07:18.360091Z"
    },
    "id": "ViDrwy-HjAs9"
   },
   "outputs": [],
   "source": [
    "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfsc88zE9upk"
   },
   "source": [
    "### Vectorize sentences from the corpus\n",
    "The steps here set everything to one case and also remove punctuation. <br>\n",
    "We do not need these for our dataset but you will need them if using other datasets with 'real' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T17:07:18.365268Z",
     "iopub.status.busy": "2020-10-30T17:07:18.364668Z",
     "iopub.status.idle": "2020-10-30T17:07:18.384471Z",
     "shell.execute_reply": "2020-10-30T17:07:18.384903Z"
    },
    "id": "2MlsXzo-ZlfK"
   },
   "outputs": [],
   "source": [
    "# We create a custom standardization function to lowercase the text and \n",
    "# remove punctuation.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "# Define the vocabulary size and number of words in a sequence.\n",
    "vocab_size = 28\n",
    "sequence_length = 7\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Set output_sequence_length length to pad all samples to same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g92LuvnyBmz1"
   },
   "source": [
    "Call `adapt` on the text dataset to create vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T17:07:18.388943Z",
     "iopub.status.busy": "2020-10-30T17:07:18.388146Z",
     "iopub.status.idle": "2020-10-30T17:07:19.626484Z",
     "shell.execute_reply": "2020-10-30T17:07:19.625897Z"
    },
    "id": "seZau_iYMPFT"
   },
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jg2z7eeHMnH-"
   },
   "source": [
    "Once the state of the layer has been adapted to represent the text corpus, the vocabulary can be accessed with `get_vocabulary()`. This function returns a list of all vocabulary tokens sorted (descending) by their frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T17:07:19.631133Z",
     "iopub.status.busy": "2020-10-30T17:07:19.630513Z",
     "iopub.status.idle": "2020-10-30T17:07:19.637921Z",
     "shell.execute_reply": "2020-10-30T17:07:19.638341Z"
    },
    "id": "jgw9pTA7MRaU"
   },
   "outputs": [],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = set([])\n",
    "for l in inverse_vocab:\n",
    "    s.update([x for x in l.split()])\n",
    "print(f'Full list: {sorted(s)}')\n",
    "print(f'Vocab size: {len(s)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOQ30Tx6KA2G"
   },
   "source": [
    "The vectorize_layer can now be used to generate vectors for each element in the `text_ds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T17:07:19.643576Z",
     "iopub.status.busy": "2020-10-30T17:07:19.642591Z",
     "iopub.status.idle": "2020-10-30T17:07:19.687389Z",
     "shell.execute_reply": "2020-10-30T17:07:19.686872Z"
    },
    "id": "yUVYrDp0araQ"
   },
   "outputs": [],
   "source": [
    "def vectorize_text(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return tf.squeeze(vectorize_layer(text))\n",
    "\n",
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YyH_SYzB72p"
   },
   "source": [
    "### Obtain sequences from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFUQLX0_KaRC"
   },
   "source": [
    "You now have a `tf.data.Dataset` of integer encoded sentences. To prepare the dataset for training a Word2Vec model, flatten the dataset into a list of sentence vector sequences. This step is required as you would iterate over each sentence in the dataset to produce positive and negative examples. \n",
    "\n",
    "Note: Since the `generate_training_data()` defined earlier uses non-TF python/numpy functions, you could also use a `tf.py_function` or `tf.numpy_function` with `tf.data.Dataset.map()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T17:07:19.691533Z",
     "iopub.status.busy": "2020-10-30T17:07:19.690896Z",
     "iopub.status.idle": "2020-10-30T17:07:22.157416Z",
     "shell.execute_reply": "2020-10-30T17:07:22.156779Z"
    },
    "id": "sGXoOh9y11pM"
   },
   "outputs": [],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDc4riukLTqg"
   },
   "source": [
    "Take a look at few examples from `sequences`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T17:07:22.162853Z",
     "iopub.status.busy": "2020-10-30T17:07:22.162050Z",
     "iopub.status.idle": "2020-10-30T17:07:22.164751Z",
     "shell.execute_reply": "2020-10-30T17:07:22.165148Z"
    },
    "id": "WZf1RIbB2Dfb"
   },
   "outputs": [],
   "source": [
    "for seq in sequences[:5]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDzSOjNwCWNh"
   },
   "source": [
    "### Generate training examples from sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BehvYr-nEKyY"
   },
   "source": [
    "`sequences` is now a list of int encoded sentences. Just call the `generate_training_data()` function defined earlier to generate training examples for the Word2Vec model. To recap, the function iterates over each word from each sequence to collect positive and negative context words. Length of target, contexts and labels should be same, representing the total number of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T17:07:22.170199Z",
     "iopub.status.busy": "2020-10-30T17:07:22.169256Z",
     "iopub.status.idle": "2020-10-30T17:07:49.123514Z",
     "shell.execute_reply": "2020-10-30T17:07:49.123995Z"
    },
    "id": "44DJ22M6nX5o",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences, \n",
    "    window_size=2, \n",
    "    num_ns=4, \n",
    "    vocab_size=vocab_size, \n",
    "    seed=SEED)\n",
    "print(len(targets), len(contexts), len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97PqsusOFEpc"
   },
   "source": [
    "### Configure the dataset for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jnFVySViQTj"
   },
   "source": [
    "To perform efficient batching for the potentially large number of training examples, use the `tf.data.Dataset` API. After this step, you would have a `tf.data.Dataset` object of `(target_word, context_word), (label)` elements to train your Word2Vec model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T17:07:49.152773Z",
     "iopub.status.busy": "2020-10-30T17:07:49.142514Z",
     "iopub.status.idle": "2020-10-30T17:07:53.921555Z",
     "shell.execute_reply": "2020-10-30T17:07:53.921951Z"
    },
    "id": "nbu8PxPSnVY2"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyrNX6Fs6K3F"
   },
   "source": [
    "Add `cache()` and `prefetch()` to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T17:07:53.926663Z",
     "iopub.status.busy": "2020-10-30T17:07:53.925694Z",
     "iopub.status.idle": "2020-10-30T17:07:53.929711Z",
     "shell.execute_reply": "2020-10-30T17:07:53.929268Z"
    },
    "id": "Y5Ueg6bcFPVL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1S-CmUMszyEf"
   },
   "source": [
    "## Model and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oc7kTbiwD9sy"
   },
   "source": [
    "### Subclassed Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jvr9pM1G1sQN"
   },
   "source": [
    "Use the [Keras Subclassing API](https://www.tensorflow.org/guide/keras/custom_layers_and_models) to define your Word2Vec model with the following layers:\n",
    "\n",
    "\n",
    "* `target_embedding`: A `tf.keras.layers.Embedding` layer which looks up the embedding of a word when it appears as a target word. The number of parameters in this layer are `(vocab_size * embedding_dim)`.\n",
    "* `context_embedding`: Another `tf.keras.layers.Embedding` layer which looks up the embedding of a word when it appears as a context word. The number of parameters in this layer are the same as those in `target_embedding`, i.e. `(vocab_size * embedding_dim)`.\n",
    "* `dots`: A `tf.keras.layers.Dot` layer that computes the dot product of target and context embeddings from a training pair.\n",
    "* `flatten`: A `tf.keras.layers.Flatten` layer to flatten the results of `dots` layer into logits.\n",
    "\n",
    "With the sublassed model, you can define the `call()` function that accepts `(target, context)` pairs which can then be passed into their corresponding embedding layer. Reshape the `context_embedding` to perform a dot product with `target_embedding` and return the flattened result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiAwuIqqw7-7"
   },
   "source": [
    "Key point: The `target_embedding` and `context_embedding` layers can be shared as well. You could also use a concatenation of both embeddings as the final Word2Vec embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T17:07:53.936865Z",
     "iopub.status.busy": "2020-10-30T17:07:53.936163Z",
     "iopub.status.idle": "2020-10-30T17:07:53.938203Z",
     "shell.execute_reply": "2020-10-30T17:07:53.938618Z"
    },
    "id": "i9ec-sS6xd8Z"
   },
   "outputs": [],
   "source": [
    "class Word2Vec(Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = Embedding(vocab_size, \n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\", )\n",
    "    self.context_embedding = Embedding(vocab_size, \n",
    "                                       embedding_dim, \n",
    "                                       input_length=num_ns+1)\n",
    "    self.dots = Dot(axes=(3,2))\n",
    "    self.flatten = Flatten()\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    we = self.target_embedding(target)\n",
    "    ce = self.context_embedding(context)\n",
    "    dots = self.dots([ce, we])\n",
    "    return self.flatten(dots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RLKz9LFECXu"
   },
   "source": [
    "### Define loss function and compile model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3Md-9QanqBM"
   },
   "source": [
    "For simplicity, you can use `tf.keras.losses.CategoricalCrossEntropy` as an alternative to the negative sampling loss. If you would like to write your own custom loss function, you can also do so as follows:\n",
    "\n",
    "``` python\n",
    "def custom_loss(x_logit, y_true):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)\n",
    "```\n",
    "\n",
    "It's time to build your model! Instantiate your Word2Vec class with an embedding dimension of 128 (you could experiment with different values). Compile the model with the `tf.keras.optimizers.Adam` optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T17:07:53.944753Z",
     "iopub.status.busy": "2020-10-30T17:07:53.944017Z",
     "iopub.status.idle": "2020-10-30T17:07:53.970026Z",
     "shell.execute_reply": "2020-10-30T17:07:53.969543Z"
    },
    "id": "ekQg_KbWnnmQ"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3MUMrluqNX2"
   },
   "source": [
    "Also define a callback to log training statistics for [Neptune](https://neptune.ai/blog/how-to-monitor-machine-learning-and-deep-learning-experiments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(weights, vocab):\n",
    "    X = []\n",
    "    X_vocab = []\n",
    "    # Align the words and weights\n",
    "    for index, word in enumerate(vocab):\n",
    "        if  index == 0: continue # skip 0, it's padding.\n",
    "        X.append(weights[index]) \n",
    "        X_vocab.append(word)\n",
    "    return(X, X_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(word, X, X_vocab, vocab):       \n",
    "    # Create a dict to find word index\n",
    "    vocab_d = {}\n",
    "    for i, w in enumerate(X_vocab):\n",
    "        vocab_d[w] = i\n",
    "        \n",
    "    # Get the similarity for the word\n",
    "    y = X[vocab_d[word]].reshape(1, -1)\n",
    "    res = cosine_similarity(X, y)\n",
    "    df = pd.DataFrame(columns=['word', 'sim'])\n",
    "    df['word'], df['sim']= vocab[1:], res\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3d_viz(X, X_vocab):\n",
    "    \n",
    "    pca = PCA(n_components=3)\n",
    "    pca_embed = pca.fit_transform(X)\n",
    "    \n",
    "    df = pd.DataFrame(columns=['x', 'y', 'z', 'word'])\n",
    "    df['x'], df['y'], df['z'], df['word'] = pca_embed[:,0], pca_embed[:,1], pca_embed[:,2], X_vocab\n",
    "    fig = px.scatter_3d(df, x='x', y='y', z='z', color='word')\n",
    "    return(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T17:07:53.974964Z",
     "iopub.status.busy": "2020-10-30T17:07:53.974162Z",
     "iopub.status.idle": "2020-10-30T17:07:53.976583Z",
     "shell.execute_reply": "2020-10-30T17:07:53.976113Z"
    },
    "id": "9d-ftBCeEZIR"
   },
   "outputs": [],
   "source": [
    "class TrackLossAndSimilarity(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        neptune.log_metric('loss', logs[\"loss\"])\n",
    "        neptune.log_metric('accuracy', logs[\"accuracy\"])\n",
    "        # Log the similarity to an example word at regular intervals\n",
    "        if epoch%100 == 0:\n",
    "            vocab = vectorize_layer.get_vocabulary()\n",
    "            X, X_vocab = get_data(word2vec.get_layer('w2v_embedding').get_weights()[0], \n",
    "                                  vocab)\n",
    "            check_word = 'bbb'\n",
    "            sim_df = get_similarity(check_word, X, X_vocab, vocab)\n",
    "            sim_fig = get_3d_viz(X, X_vocab)\n",
    "            log_chart(f'3d-plot-epoch{epoch}', sim_fig)\n",
    "            log_table(f'similarity-{check_word}-epoch{epoch}', sim_df.sort_values('sim', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5wEBotlGZ7B"
   },
   "source": [
    "Train the model with `dataset` prepared above for some number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-30T17:07:53.980785Z",
     "iopub.status.busy": "2020-10-30T17:07:53.980142Z",
     "iopub.status.idle": "2020-10-30T17:08:01.532855Z",
     "shell.execute_reply": "2020-10-30T17:08:01.533318Z"
    },
    "id": "gmC1BJalEZIY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2vec.fit(dataset, epochs=1000, callbacks=[TrackLossAndSimilarity()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
